Задача от компании "Информационная культура", которую я решал в рамках институтской научно-исследовательской практики.

Необходимо было разработать метод поиска сайтов организаций из списка.



<h1>Отчет</h1>

<h2>Постановка задачи</h2>
В рамках научно-производственной практики была предложена работа в текущем проекте компании «Информационная культура». Непосредственным заданием было найти адреса сайтов некоммерческих организаций (НКО), собранных в имеющейся базе. Задача характеризуется следующими особенностями:
•	Из всех имеющихся в базе НКО (более 200 тысяч), адреса сайтов известны только для около 1000 организаций (менее 0,5%). При этом они могут быть недостоверными. Таким образом, задача поиска распространяется на все организации.
•	Некоторые НКО могут не иметь своего сайта. Однако существует большое количество сайтов-агрегаторов, на которых собрана информация о множестве организаций, даже если у тех нет своих сайтов. Необходимо уметь отличать такие страницы от действительных сайтов НКО.
 
<h2>Методы решения</h2>
<h3>1.	Парсинг Яндекса</h3>
Первоначально для поиска сайтов было предложено анализировать результаты выдачи Яндекса на поисковые запросы, содержащие названия организаций. Такой подход налагает определенные ограничения, а именно – необходимость использования сервиса Яндекс.XML для санкционированной отправки запросов поисковику и получения структурированных ответов от него в автоматическом режиме. Запросы должны отправляться с собственного сайта пользователя, зарегистрированного в Яндекс.XML. Возможность отправки таких запросов регулируется количеством так называемых лимитов, выдаваемых сервисом Яндекс.XML сайтам пользователя.<br>
Была предпринята попытка разработки такого сайта с необходимым функционалом для отправки запросов с использованием платформы Google App Engine и проведена его регистрация в Яндекс.XML. Однако лимитов для совершения запросов с сайта сервисом выделено не было, что оказалось вполне предсказуемо.<br>
Ввиду отсутствия ресурсов для дальнейшей разработки парсера Яндекса либо альтернативных идей решения проблемы, было решено сосредоточиться на более конкретной второстепенной задаче по идентификации сайтов НКО.
<h3>2.	Идентификация сайтов НКО</h3>
Исходя из предположения, что в результате поиска получен некоторый список сайтов, необходимо распознать среди них настоящий сайт некоторой НКО. Эта задача подразумевает проведение анализа содержимого веб-страниц. В интерактивной среде Jupyter IPython был написан код по скрейпингу и разбору содержимого html-страниц. При этом основополагающей была идея о том, что на принадлежность сайта некоторой НКО будет указывать информация, содержащаяся в тэге <footer> сайта (либо в его аналоге). Таким образом не будет ложно распознан какой-либо сайт-агрегатор, имеющий страницу с упоминанием данной организации. Этот подход также имеет ряд потенциальных проблем: страница может не иметь четко определенного «футера» или «места подписи», либо оно может быть пустым; название или форма организации на сайте может быть указана иначе, чем в имеющейся базе (использование аббревиатур, сокращений, ошибки и т. д.). 

<h2>Полученные результаты и возможное развитие</h2>
Ссылка на Jupyter IPython notebook с кодом программы, всеми выходными данными и комментариями:
https://github.com/entetin/opendata/blob/master/nko.ipynb
<br>
В программной среде Jupyter был написан код, который:
1)	Открывает и подготавливает для работы базу с НКО в формате json.
2)	Для веб-страниц из списка url-адресов находит тэги <footer> и извлекает их содержимое.
3)	Составляет список аббревиатур возможных форм организаций.
4)	На примере одной организации ищет ее название в «футерах» страниц из списка.
Составленная программа полностью не решает поставленную задачу по идентификации сайта НКО в списке веб-адресов, но реализует основные этапы ее решения и демонстрирует их применение на примере одной организации.<br>
Для достижения конечного результата необходимо рассмотреть проблему анализа html-страниц, содержимое которых формируется с помощью JavaScript. После этого остается реализовать конечную классификацию сайтов на основании совпадений содержимого их «футеров» с названием организации.
 
<h2>Список использованной литературы</h2>
[1]	Документация Яндекс.XML – https://tech.yandex.ru/xml/doc/dg/concepts/about-docpage/
[2]	Документация Google App Engine – https://cloud.google.com/appengine/docs/standard/python/
[3]	«Используем Яндекс.XML на примере парсера сниппета поисковой системы Яндекс» – https://semantica.in/blog/ispolzuem-yandeks-xml-na-primere-parsera-snippeta-poiskovoj-sistemy-yandeks.html
[4]	Документация библиотеки «urllib2» – https://docs.python.org/2/library/urllib2.html
[5]	«Юникод для чайников» – https://habrahabr.ru/post/135913/
[6]	Документация библиотеки «BeautifulSoup» – https://www.crummy.com/software/BeautifulSoup/bs4/doc/
